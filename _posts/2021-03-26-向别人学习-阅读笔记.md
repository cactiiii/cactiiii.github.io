---
layout: my_post
categories: [paper translate]
---

## 前沿
终于又重新捡起来写点东西的习惯了，在此记录一下学到的可能有用的零碎知识。

* google recsys 2019 论文，他们用双塔模型+softmax做召回 [Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf)
* 讲特征工程的blog:[推荐系统之特征构建](https://zhuanlan.zhihu.com/p/221783604)
* 阿里kdd 2019 优化embedding的文章，没有代码，偏理论, 感觉没啥实际意义 [Res-embedding for Deep Learning Based Click-Through Rate Prediction Modeling](https://arxiv.org/pdf/1906.10304.pdf)
* 字节2020不知道在哪里发的，讲召回模型：[Deep Retrieval: An End-to-End Learnable Structure Model for Large-Scale Recommendations](https://arxiv.org/pdf/2007.07203.pdf)，文章内容较复古，含大量公式推导，没有源码。适合借鉴想法，不适合直接复用。
* google 2013在SIGMOD的论文：[Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41318.pdf)，讲如何将推荐日志与用户行为日志进行拼接，做的事情主要集中在保障可用性上，参考意义不大。而且文中只说了join好的点击日志如何输出，没说负样本是如何输出的。。
* facebook 2014年的论文，讲gbdt+lr的，模型本身已经过时了，但其中分享的一些数据方面的知识还挺有用的：[Practical Lessons from Predicting Clicks on Ads at Facebook](https://quinonero.net/Publications/predicting-clicks-facebook.pdf)
    * 样本时效性越强，推荐效果越好，这是公认的事实。文中使用了一种叫做HashQueue的数据结构做样本拼接。
    * 在线学习需要做样本label监控，防止被攻击，这一点容易被大家忽视，当然优先级也比较低
    * 衍生出了Per-coordinate learning rate：Ad click prediction: a view from the trenches.
    * 衍伸出了Photon: Fault-tolerant and scalable joining of continuous data streams，google如何做样本拼接的
    * 特征的重要性也符合二八定律，少数特征贡献大部分的重要性
    * 历史类特征相比于上下文特征更重要。但是上下文特征对于冷启动场景是不可或缺的
    * 在均匀负采样实验中证明，样本越多效果越好
    * 负采样能够在减少样本规模的同时提升模型效果，具体原理猜测：（1）负样本有噪音（2）更好地学习了正样本。同时文中提出了简单换算公式进行ctr矫正。
* google 2013年kdd论文：[Ad Click Prediction: a View from the Trenches](https://www.researchgate.net/publication/262412214_Ad_click_prediction_a_view_from_the_trenches)
    * 提出了Per-coordinate learning rate，效果要比全局统一的learning rate好，具体原因是出现次数多的特征得到充分学习，学习率当然要小些，稀疏的特征在学习的早期学习率当然要大一些
    * 使用16 bit存小数，这个和阿里的COLD的思想类似，实现起来较困难。。。
    * 也提到了FTRL，然而它的原理至今没有弄懂
    * 负采样的作用是在不太影响效果的前提下降低样本量，采用了query级别的采样，配合样本权重设置，保证了梯度的期望保持不变，从而不会造成效果上大的影响
    * 如何评估模型效果？通过在不同的slicing上分析，可以看出模型更胜任哪些方面的工作，更不擅长哪些方面的工作，单纯看总的指标的话，容易忽略一些可发掘的细节
    * 对于ctr矫正，提出使用piecewise linear 方法，挺暴力的。应该是把数据分段，在每个小段上进行矫正
    * 使用特征hash没有收益。其实个人认为，没必要做特征hash啊
* 中科大2020年的综述性论文：Bias and Debias in Recommender System: A Survey and Future Directions
    * 显式反馈中，存在两种bias：selection bias、conformity bias
    * 隐式反馈中，也存在两种bias：exposure bias、position bias
    * 另外还有比较重要的bias：popularity bias unfairness
    * IPS(inverse propensity score)是一种简单普遍的方法
    * 或者可以单独建模，比如对于selection bias，单独首先建模用户会不会选择item的概率，再建模会给item打多少分。
    * 讲到解决方法的时候，跟说天书一样。通篇只记住两个词：IPS和casual model
    * 提到了RL可以用于解决EE问题
* facebook 2011年的论文：[The Anatomy of the Facebook Social Graph](https://arxiv.org/pdf/1111.4503.pdf)，主要是分析了下它的社交网络的一些属性
    * facebook有鼓励用户多形成好友关系的机制，触发条件是用户好友<=20
    * facebook对用户好友的上限设置为5000
    * 用户好友数的分布并不契合指数分布,这与经验不太相符
    * 好友之间的平均距离是4.7（比想象的要高一些）
    * 99%以上的用户存在于一个连通子图中
    * 提到了一个叫做local clustering coefficient的指标，个人感觉衡量好友关系的质量？
    * 人们更倾向于和年龄相仿的人成为好友，同一个国家的也一样。但是性别上没有明显区分。
* facebook 2013 kdd论文：[Representing Documents Through Their Readers](https://research.fb.com/wp-content/uploads/2016/11/representing-documents-through-their-readers.pdf)
    * 利用文章和分享文章的用户之间的关系，建立了文章到标签的一个映射，类似于主题模型，文章由词构成，每个标签也是由词构成。在视频推荐场景的参考意义不大
* linkedIn 2013 论文：[Structural Diversity in Social Recommender Systems](https://web.archive.org/web/20150919193706/http://ls13-www.cs.tu-dortmund.de/homepage/rsweb2013/papers/Huang.pdf)
    * 主要研究社交关系对用户活跃的影响
    * 参考文献中提到了比较多的社交推荐引用
    * 参考文件中提到，朋友关系/社区成员关系对社交网络推荐有比较大的影响;用户倾向于和一定数量的好友保持一致的行为
    * 此外，推荐的多样性也对推荐效果有影响，不同的文章分析的结论不一样。facebook的结论是推荐多样性能带来更多转化，进而提高好友的多样性，最终带来更多用户行为
    * 推荐系统是社交网络提升用户活跃和网络连接的工具
    * background部分没太看懂，需要后续研究下
    * 推荐的结构多样性如何衡量？通过三个指标：connected components(两两连接的最大子图)、triangle、average local node degree(节点的平均度数)
    * 结论是：推荐的结构多样性越低（connected components数量越少，triangle/average local node degree 越大）
    * 与facebook结论相反，分析原因是facebook应用场景更多是加入一个圈子，LinkedIn使用场景更多是在圈子内认识更多人
    * 用户的好友网络的结构多样性越高，用户越活跃
* PYMK: friend recommendation at myspace download 论文本身在网上找不到。。但是搜索它能搜到挺多后续的pymk方面的研究
* pymk方向2018的一篇论文：[Online Social Friend Recommendation Based on Multiple Social Network Correlation](http://www.ijatir.org/uploads/534621IJATIR16774-84.pdf)
    * 论文本身影响力不大，但是比较新
    * 推荐基于的数据有两大方向：基于拓扑和基于内容，个人理解：基于拓扑更多倾向pymk，基于内容更多倾向pyml
    * 隐私保护下的好友推荐（好友关系被视为敏感/隐私信息）
    * 文章本身价值不大，就是一篇水文。参考文献倒是可以帮助发现一些资源
* 流行度偏差方向2021年的论文，由腾讯联合大学发表：[Causal Intervention for Leveraging Popularity Bias in Recommendation](https://arxiv.org/pdf/2105.06067.pdf)
    * 流行度呈现长尾分布趋势，头部小部分item占据大部分的曝光位置。这种情况在模型训练-预测循环中又得到了放大。
    * 现有方法：Inverse Propensity Scoring（IPS），比较简单，效果一般；Causal Embedding，无法实施；Ranking Adjustment：启发式方法
    * 文章的假设：流行度分为两部分，一种是item内在属性带来的，这部分是可以加以利用的；另一部分是模型带来的，是需要加以遏制的
    * 推导过程很复杂，并且还使用了一些假设的东西。最终结论却挺简单：训练的时候，在原来模型预测的概率的基础上乘以m的r次方，就相当于考虑了流行度影响，然后预测的时候只使用模型预测的概率就可以消除流行度的负面影响
    * 往后就更扯了，预测的时候再乘上一个**预测的m的r次方**，这就成了利用了流行度的正面价值
    * 在实际使用上，这个和IPS的难度感觉相差不大
    * 好的地方是这篇文章带code：[链接](https://github.com/zyang1580/PDA)
* 关于ANN
    * NN指Nearest Neighbor，ANN指Approximate Nearest Neighbor
    * NN的挑战不仅仅在于vector的维度比较高，也在于本身item的量比较大，两两之间都计算一次相似度然后取top的方法算得太慢，耗资源太多（spark也无能为力,总是OOM）
    * ANN中基于hash方法可以分为两种，一种是data-dependent的；另一种是data-independent，典型就是局部敏感哈希（Locality-Sensitive Hashing），可以有多个HashTable，每个HashTable可以有多个HashFunction
    * 可以用于各种去重与匹配：网页/文本/图像/音乐/指纹等
    * 重要的是hash function的选择，根据不同的距离定义，hash function也不一样
        * Jaccard distance：minhash。原始的minhash擅长解决两个item的Jaccard Distance的问题，更像是一种降维的方法，如果涉及到众多item，则需要在此基础上再做LSH
        * Cosine distance：sign(V*R)，其中R是一个随机向量。正是之前研究过的Annoy的实现，hashTable就是Annoy里的树的概念
* 2018年关于cascade learning的论文：[Deep Cascade Learning](https://ieeexplore.ieee.org/ielx7/5962385/8495104/08307262.pdf)
    * cascade learning就是自下而上地逐层训练深度神经网络，这样每层训练的时候都离目标不远，不会有梯度消失的现象，因此能够在底层网络中学到更“健壮”的特征和过滤器（CNN专有）
    * 时间复杂度降低（因为会把上次训练的预测结果存储起来，下次直接用，不用再前向传播了），空间复杂度降低（网络比较大的时候，存储网络本身比存储上次训练的输出更占内存）
    * 算法的方差更小（观察到的现象），比传统方式更稳定，传统end-to-end训练更依赖初始化的好坏（方差大的后果）
    * 但是算法本身的偏差也更大，因此更适合作为一种预训练方案
* hinton 2015年的论文：[Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf)
    * bagging是一种简单的提升效果的方法，但是缺点是模型和计算量会比较大
    * 其实模型训练的目标和使用者的目标并不吻合。模型训练的目标是优化训练集上的指标，而真实目标是优化在新数据上的泛化性能。
    * 大模型预测的概率分布可以作为训练小模型的“软目标”
    * 所谓“蒸馏”，是指在softmax中引入了一个温度T,使用大模型训练小模型的时候，用一个比较大的T。原因是为了使大模型输出的概率分布更加平滑，否则有些概率比较小的分类的梯度就太小了，而这些分类的pxtr其实还是包含挺多重要信息的。
    * 小模型作预测的时候，T要设置回1
    * 在loss函数中加入一项与真实label的交叉熵损失，可以提高模型表现。但有两点需要注意：1 新加的损失权重得比较小 2 原来的损失要乘上T的平方（公式推导中假设问题T很高，logits的平均值是0)
    * 学多个大模型怎么学？直接搞可能还是成本太高，可以学一个general model，然后在它confuse的类的集合上（这个集合可能又多个）分别训练specialist model
    * 通过蒸馏，小模型能学到大模型如何泛化的知识
* hinton 1991年的论文：[Adaptive Mixture of Local Expert ](https://www.researchgate.net/publication/233806999_Adaptive_Mixture_of_Local_Expert)
    * 多个相同结构的expert,一个gating network，gating network的输出经过softmax后作为expert被选中的概率
    * 亮点是loss函数的改进：缓解了experts之间的合作，鼓励了竞争，从而模型倾向于每次只有少数个export被激活，原理是：当某个expert的loss小于所有expert loss的加权平均时，它的weight就会增长，反之亦然
* 阿里2021年召回：[Path-based Deep Network for Candidate Item Matching in Recommenders](https://arxiv.org/pdf/2105.08246.pdf)
    * 包含Direct Net，Trigger Net & Similarity Net，Bias Net
    * Bias Net 用来在训练时学position bias、temporal bias等的影响，预测时去掉（跟我之前的想法有点像，可惜啊）
    * 用TriggerNet、Similarity Net来分别学用户-互动item、互动item-目标item的关系，本质上来说是用用户的互动item作为用户的兴趣点，逐个计算与目标item之间的关联程度
    * 相比CF，能够引入用户属性作为特征，相比EBR，能够捕捉更多样的用户兴趣
    * loss 设计得挺有意思，首先是Trigger Net & Similarity Net的输出的构造，其次是预测的相似度如何转化为0～1之间的概率值，这个得看原文
    * 预测的时候怎么做？
        * 首先得先建个倒排，对每个item，先用规则划出一批候选，然后用Similarity Net打分，把前k个作为倒排value
        * 线上召回的时候，先取用户的互动item，用Trigger Net对这些item打分，保留前m个
        * 然后用m个互动item去查倒排，得到候选池，然后再用不包含bias net的相似度公式去算相似度（Direct Net在此过程生效）
* 阿里2018年模型：Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate
    * 在诸如 点击-转化 这种包含多个顺序步骤的场景中，如何准确预估点击率和转化率？
    * 点击率的预估比较经典。但转化率如何预估？首先必须在点击样本上进行训练，转化成功的是正样本，其他的是负样本。这样就带来了两个问题：
        * 如何使用？使用的时候如果在曝光样本上直接预测转化率，那么会有样本选择偏差，ssb(sample selection bias)
        * 样本量比较少
    * 因此文章提出了ctr和cvr一起预测的模型，在曝光样本上训练，同时解决了样本选择偏差和样本稀疏问题
    * 为了能够在曝光样本上预测cvr，选择了预测ctr和ctcvr两个目标，ctcvr = ctr * cvr,其中cvr的意思是在点击发生的情况下转化成功的概率
* google brain 17年的论文：[SWISH: A SELF-GATED ACTIVATION FUNCTION](https://arxiv.org/pdf/1710.05941v1.pdf?source=post_page)
    * 提出了新的激活函数：swish，公式比较简单：x * sigmoid(x)
    * 灵感来源于使用sigmoid做gate，swish的这种方式作者称为self-gating
    * 还有个带参数beta的版本： 2x * sigmoid(beta * x)，通过控制beta超参，swish-beta相当于在y=x与relu之间的非线性插值(不太懂这个名词)
    * 用的时候学习率比用relu的时候少小一点比较合适
    * swish的几个特性：无上界，有下界，非单调，光滑
        * 无上界：relu和swish都无上界，和sigmoid/tanh比起来，x>0的时候不会有饱和现象
        * 有下界：有下界意味着函数能起到正则化的作用，左极限为0意味着更强的正则化效果（可能是相比于只在0点附近为0的函数来说）
        * 非单调：swish在0点偏左的地方值为负，增加了表达能力，改善了梯度流(大意就是梯度不为0，就能产生非0的梯度,反向传播的时候不至于造成梯度消失),同时对于不同的初始化方法及学习率来说也更鲁棒
* [FITNETS: HINTS FOR THIN DEEP NETS](https://arxiv.org/pdf/1412.6550.pdf)
    * 提出了一个叫做 FitNet的网络，目的是通过模型压缩，把大模型压缩成窄而深的小模型，“深”保证了小模型的泛华性，“窄”的好处是运算量减小了
    * 基于Knowledge Distillation做的改进，KD的不足之处是：无法训练比teacher深得多的student
* facebook 17年的文章：[Language Modeling with Gated Convolutional Networks](https://arxiv.org/pdf/1612.08083.pdf)
    * github上找的一个实现（看着不一定正确）：[代码](https://github.com/anantzoid/Language-Modeling-GatedCNN/blob/d49161af7ba32c71516a27e9d2603c6364234a44/model.py#L59)
    * 搞的是个语言模型
    * 使用CNN来提取上下文信息，相比RNN来说，优点是便于并行计算，缺点是有最长上下文限制，但是作者说上下文用不着太长
    * 使用类似于LSTM中的输出门的机制，在CNN原本的输出层，支出了两条路径：一路是线性FC，一路是sigmoid(FC)，第二路称为output gate，然后两路输出结果进行element wise相乘。注意两路的FC的权重/偏置是独立的
        * 这种结构有什么优点？相比于第一路使用tanh作为激活函数的版本来说，求导后发现有一项不会有饱和现象，从而层数较深的情况下不会有梯度消失现象
    * 还使用了带bottleneck的resnet，以及adaptive softmax，以及weight normalization and gradient clipping
* 阿里20年论文：[Improving Multi-Scenario Learning to Rank in E-commerce by Exploiting Task Relationships in the Label Space](https://cs.nju.edu.cn/zlj/pdf/CIKM-2020-Li.pdf)
    * 提出的模型叫做HMOE，是在MMOE基础上改进而来，主要是为了解决多市场如何更好地在一个模型中训练的问题
    * 在多市场的情况下，直接训练一个大模型忽略了不同市场间的差异性；直接训练N个独立的小模型又忽略了市场之间的相关性
    * 这是一个典型的多任务学习（MTL）问题，不同市场间使用相同的特征空间和label空间
    * 借鉴MMOE结构，在特征空间学习不同市场的差异性和相关性；使用堆叠的模型在label空间学习不同任务的联系；同时提出了一个反向传播的方法，避免上下两个模型不能一起训练的问题
    * moe包括expert network和gate network，其中gate network使用的是softmax;文中使用的IMMOE是在moe基础上为每个task又增加了独立的MLP塔
    * 在IMMOE上又stack了一个model，主要学习在label空间中的不同市场的关联,具体做法有点类似于moe，也是用一个gate网络输出权重序列，每个权重值i对应市场i对当前市场的重要程度,注意是单向的
    * 上下两个gate原理类似，但是一个是在特征空间的选择，一个是在label空间的选择
    * 学习的时候，如果上下两个模型分开学习，就无法online learning；如果直接一起学习，则上面的模型就无法学习到domain-specific知识（就是说不同市场其实又共享上面的模型了）
    * 解决方式是控制反向传播的梯度，只回传到对应的塔内，以及gate网络中
* google 2021年的论文：[Large Dual Encoders Are Generalizable Retrievers](https://arxiv.org/pdf/2112.07899.pdf)
    * 虽然是google的，但是感觉没说啥
    * 结论是对于双塔模型来说，仅仅增加模型的规模，不用增加最终的emb的大小，就可以提升召回指标，尤其是跨域（domain）的指标。（跨域是指在其他区域的数据上训练，在本区域的数据上评估）
    * 文章讨论的是NLP领域的召回，所以预训练对于模型来说提升也比较大
* 发现一个介绍系统设计的课程[链接](https://www.educative.io/courses/grokking-the-system-design-interview?affiliate_id=5749180081373184%2F)，系统设计是个人的短板之一，所以打算花点钱看一下
    * 好像找到了一个免费的拷贝：[链接](https://akshay-iyangar.github.io/system-design/grokking-system-design/system-design-problems/instagram.html)，cool！
* facebook 2021年的论文：Que2Search: Fast and Accurate Query and Document Understanding for Search at Facebook
    * 原来在facebook也有这种现象：离线提升5%，在线实验只有4%
    * 用了XLM模型来理解自然语言，这是个什么模型？要不要看一下？
    * 双塔模型主要用于embedding-based retrieval以及粗排模型，其实是比较类似的，最新的好像还有什么Siamese network与w&d相结合的手段。。
    * 双塔模型的负样本是不是也比较重要？
    * 每个塔其实都是几个base model的fusion，处理自然语言用XLM，处理图像用预训练的模型，然后使用simple attention fusion得到最终embedding。
    * 在simple attention fusion的基础上，还用了gradient blending，就是使用多个loss函数，增强模型鲁棒性（使模型学习到在某些base model失效的情况下如何工作）
    * 对于预训练的base model，采用了比较低的learning rate
    * 对于document tower，还支出了一个分类任务辅助训练，采用multi label cross entropy损失函数，与原任务形成了一个multi task learning
    * 样本是如何构造的？正样本是根据业务理解，设置了过滤规则的pair对：用户点击进入商品详情，并与卖家交谈。负样本分两部分，第一部分是batch内的其他document，使用scaled multi-class cross-entropy loss，第二部分是batch内选的最hard的一个负样本，使用margin rank loss，其中的关键是：1 scaled对于收敛起重要作用 2 第二阶段必须在第一阶段已经收敛后再开始
    * ANN召回不是万能的，可以配合业务理解添加一些策略，例如文中就设置了位置/社交/分类上的限制(所以说，不要羞于使用策略）
    * 还可以这样？部署的时候，首屏使用对ANN相对保守的参数，保延迟牺牲一些精度，第二屏开始使用正常的参数。这样据说显著降低了硬件资源的消耗
    * 召回是不是数量越多越好？不是，召回的数据多了以后，排序模型可能无法处理多出来的噪音item，原因是排序模型与召回模型的不一致问题（inconsistency），这也是一个研究方向
    * 模型的可解释性：使用attention weight看特征的重要性；feature ablation，即让某个feature对应的输入置为默认值，其他都不变，然后根据输出的变化来计算feature importance
    * 未完全明了的关键词：multi-label cross-entropy，multi-class cross entropy，gradient blending，deep sets fusion, feature ablation

