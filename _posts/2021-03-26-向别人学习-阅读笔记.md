---
layout: my_post
categories: [paper translate]
---

## 前沿
终于又重新捡起来写点东西的习惯了，在此记录一下学到的可能有用的零碎知识。

* google recsys 2019 论文，他们用双塔模型+softmax做召回 [Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/6c8a86c981a62b0126a11896b7f6ae0dae4c3566.pdf)
* 讲特征工程的blog:[推荐系统之特征构建](https://zhuanlan.zhihu.com/p/221783604)
* 阿里kdd 2019 优化embedding的文章，没有代码，偏理论, 感觉没啥实际意义 [Res-embedding for Deep Learning Based Click-Through Rate Prediction Modeling](https://arxiv.org/pdf/1906.10304.pdf)
* 字节2020不知道在哪里发的，讲召回模型：[Deep Retrieval: An End-to-End Learnable Structure Model for Large-Scale Recommendations](https://arxiv.org/pdf/2007.07203.pdf)，文章内容较复古，含大量公式推导，没有源码。适合借鉴想法，不适合直接复用。
* google 2013在SIGMOD的论文：[Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41318.pdf)，讲如何将推荐日志与用户行为日志进行拼接，做的事情主要集中在保障可用性上，参考意义不大。而且文中只说了join好的点击日志如何输出，没说负样本是如何输出的。。
* facebook 2014年的论文，讲gbdt+lr的，模型本身已经过时了，但其中分享的一些数据方面的知识还挺有用的：[Practical Lessons from Predicting Clicks on Ads at Facebook](https://quinonero.net/Publications/predicting-clicks-facebook.pdf)
    * 样本时效性越强，推荐效果越好，这是公认的事实。文中使用了一种叫做HashQueue的数据结构做样本拼接。
    * 在线学习需要做样本label监控，防止被攻击，这一点容易被大家忽视，当然优先级也比较低
    * 衍生出了Per-coordinate learning rate：Ad click prediction: a view from the trenches.
    * 衍伸出了Photon: Fault-tolerant and scalable joining of continuous data streams，google如何做样本拼接的
    * 特征的重要性也符合二八定律，少数特征贡献大部分的重要性
    * 历史类特征相比于上下文特征更重要。但是上下文特征对于冷启动场景是不可或缺的
    * 在均匀负采样实验中证明，样本越多效果越好
    * 负采样能够在减少样本规模的同时提升模型效果，具体原理猜测：（1）负样本有噪音（2）更好地学习了正样本。同时文中提出了简单换算公式进行ctr矫正。
* google 2013年kdd论文：[Ad Click Prediction: a View from the Trenches](https://www.researchgate.net/publication/262412214_Ad_click_prediction_a_view_from_the_trenches)
    * 提出了Per-coordinate learning rate，效果要比全局统一的learning rate好，具体原因是出现次数多的特征得到充分学习，学习率当然要小些，稀疏的特征在学习的早期学习率当然要大一些
    * 使用16 bit存小数，这个和阿里的COLD的思想类似，实现起来较困难。。。
    * 也提到了FTRL，然而它的原理至今没有弄懂
    * 负采样的作用是在不太影响效果的前提下降低样本量，采用了query级别的采样，配合样本权重设置，保证了梯度的期望保持不变，从而不会造成效果上大的影响
    * 如何评估模型效果？通过在不同的slicing上分析，可以看出模型更胜任哪些方面的工作，更不擅长哪些方面的工作，单纯看总的指标的话，容易忽略一些可发掘的细节
    * 对于ctr矫正，提出使用piecewise linear 方法，挺暴力的。应该是把数据分段，在每个小段上进行矫正
    * 使用特征hash没有收益。其实个人认为，没必要做特征hash啊
* 中科大2020年的综述性论文：Bias and Debias in Recommender System: A Survey and Future Directions
    * 显式反馈中，存在两种bias：selection bias、conformity bias
    * 隐式反馈中，也存在两种bias：exposure bias、position bias
    * 另外还有比较重要的bias：popularity bias unfairness
    * IPS(inverse propensity score)是一种简单普遍的方法
    * 或者可以单独建模，比如对于selection bias，单独首先建模用户会不会选择item的概率，再建模会给item打多少分。
    * 讲到解决方法的时候，跟说天书一样。通篇只记住两个词：IPS和casual model
    * 提到了RL可以用于解决EE问题
* facebook 2011年的论文：[The Anatomy of the Facebook Social Graph](https://arxiv.org/pdf/1111.4503.pdf)，主要是分析了下它的社交网络的一些属性
    * facebook有鼓励用户多形成好友关系的机制，触发条件是用户好友<=20
    * facebook对用户好友的上限设置为5000
    * 用户好友数的分布并不契合指数分布,这与经验不太相符
    * 好友之间的平均距离是4.7（比想象的要高一些）
    * 99%以上的用户存在于一个连通子图中
    * 提到了一个叫做local clustering coefficient的指标，个人感觉衡量好友关系的质量？
    * 人们更倾向于和年龄相仿的人成为好友，同一个国家的也一样。但是性别上没有明显区分。
* facebook 2013 kdd论文：[Representing Documents Through Their Readers](https://research.fb.com/wp-content/uploads/2016/11/representing-documents-through-their-readers.pdf)
    * 利用文章和分享文章的用户之间的关系，建立了文章到标签的一个映射，类似于主题模型，文章由词构成，每个标签也是由词构成。在视频推荐场景的参考意义不大
* linkedIn 2013 论文：[Structural Diversity in Social Recommender Systems](https://web.archive.org/web/20150919193706/http://ls13-www.cs.tu-dortmund.de/homepage/rsweb2013/papers/Huang.pdf)
    * 主要研究社交关系对用户活跃的影响
    * 参考文献中提到了比较多的社交推荐引用
    * 参考文件中提到，朋友关系/社区成员关系对社交网络推荐有比较大的影响;用户倾向于和一定数量的好友保持一致的行为
    * 此外，推荐的多样性也对推荐效果有影响，不同的文章分析的结论不一样。facebook的结论是推荐多样性能带来更多转化，进而提高好友的多样性，最终带来更多用户行为
    * 推荐系统是社交网络提升用户活跃和网络连接的工具
    * background部分没太看懂，需要后续研究下
    * 推荐的结构多样性如何衡量？通过三个指标：connected components(两两连接的最大子图)、triangle、average local node degree(节点的平均度数)
    * 结论是：推荐的结构多样性越低（connected components数量越少，triangle/average local node degree 越大）
    * 与facebook结论相反，分析原因是facebook应用场景更多是加入一个圈子，LinkedIn使用场景更多是在圈子内认识更多人
    * 用户的好友网络的结构多样性越高，用户越活跃
* PYMK: friend recommendation at myspace download 论文本身在网上找不到。。但是搜索它能搜到挺多后续的pymk方面的研究
* pymk方向2018的一篇论文：[Online Social Friend Recommendation Based on Multiple Social Network Correlation](http://www.ijatir.org/uploads/534621IJATIR16774-84.pdf)
    * 论文本身影响力不大，但是比较新
    * 推荐基于的数据有两大方向：基于拓扑和基于内容，个人理解：基于拓扑更多倾向pymk，基于内容更多倾向pyml
    * 隐私保护下的好友推荐（好友关系被视为敏感/隐私信息）
    * 文章本身价值不大，就是一篇水文。参考文献倒是可以帮助发现一些资源
* 流行度偏差方向2021年的论文，由腾讯联合大学发表：[Causal Intervention for Leveraging Popularity Bias in Recommendation](https://arxiv.org/pdf/2105.06067.pdf)
    * 流行度呈现长尾分布趋势，头部小部分item占据大部分的曝光位置。这种情况在模型训练-预测循环中又得到了放大。
    * 现有方法：Inverse Propensity Scoring（IPS），比较简单，效果一般；Causal Embedding，无法实施；Ranking Adjustment：启发式方法
    * 文章的假设：流行度分为两部分，一种是item内在属性带来的，这部分是可以加以利用的；另一部分是模型带来的，是需要加以遏制的
    * 推导过程很复杂，并且还使用了一些假设的东西。最终结论却挺简单：训练的时候，在原来模型预测的概率的基础上乘以m的r次方，就相当于考虑了流行度影响，然后预测的时候只使用模型预测的概率就可以消除流行度的负面影响
    * 往后就更扯了，预测的时候再乘上一个**预测的m的r次方**，这就成了利用了流行度的正面价值
    * 在实际使用上，这个和IPS的难度感觉相差不大
    * 好的地方是这篇文章带code：[链接](https://github.com/zyang1580/PDA)
* 关于ANN
    * NN指Nearest Neighbor，ANN指Approximate Nearest Neighbor
    * NN的挑战不仅仅在于vector的维度比较高，也在于本身item的量比较大，两两之间都计算一次相似度然后取top的方法算得太慢，耗资源太多（spark也无能为力,总是OOM）
    * ANN中基于hash方法可以分为两种，一种是data-dependent的；另一种是data-independent，典型就是局部敏感哈希（Locality-Sensitive Hashing），可以有多个HashTable，每个HashTable可以有多个HashFunction
    * 可以用于各种去重与匹配：网页/文本/图像/音乐/指纹等
    * 重要的是hash function的选择，根据不同的距离定义，hash function也不一样
        * Jaccard distance：minhash。原始的minhash擅长解决两个item的Jaccard Distance的问题，更像是一种降维的方法，如果涉及到众多item，则需要在此基础上再做LSH
        * Cosine distance：sign(V*R)，其中R是一个随机向量。正是之前研究过的Annoy的实现，hashTable就是Annoy里的树的概念
* 2018年关于cascade learning的论文：[Deep Cascade Learning](https://ieeexplore.ieee.org/ielx7/5962385/8495104/08307262.pdf)
    * cascade learning就是自下而上地逐层训练深度神经网络，这样每层训练的时候都离目标不远，不会有梯度消失的现象，因此能够在底层网络中学到更“健壮”的特征和过滤器（CNN专有）
    * 时间复杂度降低（因为会把上次训练的预测结果存储起来，下次直接用，不用再前向传播了），空间复杂度降低（网络比较大的时候，存储网络本身比存储上次训练的输出更占内存）
    * 算法的方差更小（观察到的现象），比传统方式更稳定，传统end-to-end训练更依赖初始化的好坏（方差大的后果）
    * 但是算法本身的偏差也更大，因此更适合作为一种预训练方案
* hinton 2015年的论文：[Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf)
    * bagging是一种简单的提升效果的方法，但是缺点是模型和计算量会比较大
    * 其实模型训练的目标和使用者的目标并不吻合。模型训练的目标是优化训练集上的指标，而真实目标是优化在新数据上的泛化性能。
    * 大模型预测的概率分布可以作为训练小模型的“软目标”
    * 所谓“蒸馏”，是指在softmax中引入了一个温度T,使用大模型训练小模型的时候，用一个比较大的T。原因是为了使大模型输出的概率分布更加平滑，否则有些概率比较小的分类的梯度就太小了，而这些分类的pxtr其实还是包含挺多重要信息的。
    * 小模型作预测的时候，T要设置回1
    * 在loss函数中加入一项与真实label的交叉熵损失，可以提高模型表现。但有两点需要注意：1 新加的损失权重得比较小 2 原来的损失要乘上T的平方（公式推导中假设问题T很高，logits的平均值是0)
    * 学多个大模型怎么学？直接搞可能还是成本太高，可以学一个general model，然后在它confuse的类的集合上（这个集合可能又多个）分别训练specialist model
    * 通过蒸馏，小模型能学到大模型如何泛化的知识
* hinton 1991年的论文：[Adaptive Mixture of Local Expert ](https://www.researchgate.net/publication/233806999_Adaptive_Mixture_of_Local_Expert)
    * 多个相同结构的expert,一个gating network，gating network的输出经过softmax后作为expert被选中的概率
    * 亮点是loss函数的改进：缓解了experts之间的合作，鼓励了竞争，从而模型倾向于每次只有少数个export被激活，原理是：当某个expert的loss小于所有expert loss的加权平均时，它的weight就会增长，反之亦然
* 阿里2021年召回：[Path-based Deep Network for Candidate Item Matching in Recommenders](https://arxiv.org/pdf/2105.08246.pdf)
    * 包含Direct Net，Trigger Net & Similarity Net，Bias Net
    * Bias Net 用来在训练时学position bias、temporal bias等的影响，预测时去掉（跟我之前的想法有点像，可惜啊）
    * 用TriggerNet、Similarity Net来分别学用户-互动item、互动item-目标item的关系，本质上来说是用用户的互动item作为用户的兴趣点，逐个计算与目标item之间的关联程度
    * 相比CF，能够引入用户属性作为特征，相比EBR，能够捕捉更多样的用户兴趣
    * loss 设计得挺有意思，首先是Trigger Net & Similarity Net的输出的构造，其次是预测的相似度如何转化为0～1之间的概率值，这个得看原文
    * 预测的时候怎么做？
        * 首先得先建个倒排，对每个item，先用规则划出一批候选，然后用Similarity Net打分，把前k个作为倒排value
        * 线上召回的时候，先取用户的互动item，用Trigger Net对这些item打分，保留前m个
        * 然后用m个互动item去查倒排，得到候选池，然后再用不包含bias net的相似度公式去算相似度（Direct Net在此过程生效）
* 阿里2018年模型：Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate
    * 在诸如 点击-转化 这种包含多个顺序步骤的场景中，如何准确预估点击率和转化率？
    * 点击率的预估比较经典。但转化率如何预估？首先必须在点击样本上进行训练，转化成功的是正样本，其他的是负样本。这样就带来了两个问题：
        * 如何使用？使用的时候如果在曝光样本上直接预测转化率，那么会有样本选择偏差，ssb(sample selection bias)
        * 样本量比较少
    * 因此文章提出了ctr和cvr一起预测的模型，在曝光样本上训练，同时解决了样本选择偏差和样本稀疏问题
    * 为了能够在曝光样本上预测cvr，选择了预测ctr和ctcvr两个目标，ctcvr = ctr * cvr,其中cvr的意思是在点击发生的情况下转化成功的概率



