---
layout: my_post
categories: [interview]
---

##什么是KL散度？

##什么是算术平均、几何平均、调和平均？几何平均相比算数平均有什么优点

##不同的激活函数：Sigmoid、tanh、ReLU、ReLU6及变体P-R-Leaky、ELU、SELU、Swish、Mish、Maxout、hard-sigmoid、hard-swish、leaky relu(LRELU)、prelu、softplus

##平衡二叉树你会实现吗？

##离线auc提升与线上指标提升的比例关系？
目前的想法是1:10的关系，比如离线提升0.2pp，线上相关指标提升2%

##介绍下dropout？

##介绍下batch normalization? layernorm?

##训练数据有多少？qps有多少？

##tanh(双曲正切）公式怎么写？ sinh(双曲正弦）cosh(双曲余弦)呢 sigmoid呢

##讲一下batch normalization

##lstm公式推导一下

##rnn反向传播的公式推导一下

##常见的几种优化方法？

##参数的三种更新方式：batch mini-batch sgd 的区别是什么？

##attentional transformer是啥？

##a bottleneck structure是啥

## hierarchical softmax和adaptive softmax

## weight normalization是啥？

## Kaiming initialization?

## X, Y独立同分布 ～U(0, s)，求 E(max(X, Y))?

## X, Y独立同分布 ～N(e, d^2)，求 E(max(X, Y))?

## 多模态学习？

## RNN如何做反向传播

## 几种常见的优化方式：momentum adagrad adam等？

## 几个数据？
dau 总共6000w，br 2000w，in 1000w， spa 1300w；时长 br/in 60min，spa 50min
训练数据 每天84条训练数  evr:0.45 fpr:0.32 ctr:0.1 ltr:0.05
qps 高峰1.6w，低峰期4k
精排模型dense参数有1300w个，sparse slots总共有223个，加起来有5128维


## 雅可比矩阵
多个y，多个x，分别两两求偏导，横坐标对应y，纵坐标对应x

## 交叉熵怎么理解？
* 方法1: 与极大似然估计等价
* 方法2: kl散度(P|Q) = H(P,Q) - H(P)，P是真实分布，物理意义是使用基于Q的编码来编码来自P的样本，要多使用多少个bit。其中H(P, Q)就是交叉熵，我们要优化真实分布P和拟合分布Q之间的KL散度，而H(P)是保持不变的，因此可以直接优化H(P, Q)

## 互信息的公式：
* I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y)，注意这里的H(X,Y)是联合熵，与交叉熵用的一样的表达，但语境不同，代表的含义也不同
* 公式展开后是p(x,y)log(p(x,y)/p(x)/p(y))的求和

## Z-score Normalization 是啥？
* 就是减去均值，然后除以标准差
* 区别于min-max归一化
