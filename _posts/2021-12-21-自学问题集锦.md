---
layout: my_post
categories: [interview]
---

##什么是KL散度？

##什么是算术平均、几何平均、调和平均？几何平均相比算数平均有什么优点

##不同的激活函数：Sigmoid、tanh、ReLU、ReLU6及变体P-R-Leaky、ELU、SELU、Swish、Mish、Maxout、hard-sigmoid、hard-swish、leaky relu(LRELU)、prelu、softplus

##平衡二叉树你会实现吗？

##离线auc提升与线上指标提升的比例关系？
目前的想法是1:10的关系，比如离线提升0.2pp，线上相关指标提升2%

##介绍下dropout？

##介绍下batch normalization? layernorm?

##训练数据有多少？qps有多少？

##tanh(双曲正切）公式怎么写？ sinh(双曲正弦）cosh(双曲余弦)呢 sigmoid呢

##讲一下batch normalization

##lstm公式推导一下

##rnn反向传播的公式推导一下

##常见的几种优化方法？

##参数的三种更新方式：batch mini-batch sgd 的区别是什么？

##attentional transformer是啥？

##a bottleneck structure是啥

## hierarchical softmax和adaptive softmax

## weight normalization是啥？

## Kaiming initialization?

## X, Y独立同分布 ～U(0, s)，求 E(max(X, Y))?

## X, Y独立同分布 ～N(e, d^2)，求 E(max(X, Y))?

## 多模态学习？

## RNN如何做反向传播

## 几种常见的优化方式：momentum adagrad adam等？

## 几个数据？
dau 总共6000w，br 2000w，in 1000w， spa 1300w；时长 br/in 60min，spa 50min
训练数据 每天84条训练数  evr:0.45 fpr:0.32 ctr:0.1 ltr:0.05
qps 高峰1.6w，低峰期4k
精排模型dense参数有1300w个，sparse slots总共有223个，加起来有5128维


## 雅可比矩阵
多个y，多个x，分别两两求偏导，横坐标对应y，纵坐标对应x

## 交叉熵怎么理解？
* 方法1: 与极大似然估计等价
* 方法2: kl散度(P|Q) = H(P,Q) - H(P)，P是真实分布，物理意义是使用基于Q的编码来编码来自P的样本，要多使用多少个bit。其中H(P, Q)就是交叉熵，我们要优化真实分布P和拟合分布Q之间的KL散度，而H(P)是保持不变的，因此可以直接优化H(P, Q)

## 互信息的公式：
* I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y)，注意这里的H(X,Y)是联合熵，与交叉熵用的一样的表达，但语境不同，代表的含义也不同
* 公式展开后是p(x,y)log(p(x,y)/p(x)/p(y))的求和

## 交叉熵和互信息
* 交叉熵/KL散度衡量的是两个分布的差异，就是事件有哪些是确定的，只是概率有差异
* 互信息衡量的是两个随机变量，互相依赖的程度。两个随机变量可以对应不同的事件。同时两个变量的联合分布，又是一些确定事件集合上的概率分布，因此I(X;Y)= KL散度(p(x,y) | p(x)\*p(y))

## Z-score Normalization 是啥？
* 就是减去均值，然后除以标准差
* 区别于min-max归一化

## 一篇讲逻辑回归的博客：[【机器学习】逻辑回归](https://zhuanlan.zhihu.com/p/74874291)
* sigmoid函数是怎么来的？出发点是使用线性模型回归对数几率，经过推导得到sigmoid表达式
* 代价函数就是对数似然函数
* 优化一般用梯度下降法，因为牛顿法涉及到求海森矩阵的逆矩阵
* 关于正则化，如何理解权重越小，模型就越简单？模型复杂的时候，输入值稍一变动，预测值就变动，这种情况w很小的时候比较难出现
* L1正则相当于给模型参数引入了拉普拉斯令均值先验分布，L2正则化相当于引入了零均值正态分布
* L2正则化引入loss函数的，其实是L2范数（L2 norm）的平方

## dropout不管是训练时对激活值除以p，还是预测时对W乘以p，本质上都是对激活值进行处理。。

## 根据史塔西的文章[先入为主：将先验知识注入推荐模型](https://zhuanlan.zhihu.com/p/442845759)来解释双塔粗排的一些网络设计
* 先验的重要特征用在两个地方：加在浅层（四大id拼在倒数第二层）以及当裁判（特征层面上的LHUC）
* 加在浅层的bucket等信息也能起到一定的debias作用
* evr向cstr等目标手动信息迁移

## 连续值按照分位数分桶也可以叫做：等频归一化

## 经典对比loss里为啥要设置margin？
* 不设置margin的话模型会走捷径：把所有的输入都映射到相同的输出上，这样loss就永远为0

## Metric Learning (http://contrib.scikit-learn.org/metric-learn/introduction.html)
* 什么是Metric Learning？
    * 全称Distance Metric Learning（距离元学习？），主要是学习points之间的距离。
* 监督学习和弱监督学习（weakly supervised learning）
    * 弱监督学习是指样本没有直接的label，而是有正样本对和负样本对，模型把正样本对的距离拉近，负样本对的距离拉远
* 输入是由feature组成的vector，其实可以理解为原始embedding。将原始embedding投影到一个新的空间中，在新的空间中的距离是符合学习目标的
