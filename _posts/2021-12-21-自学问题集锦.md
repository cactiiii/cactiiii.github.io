---
layout: my_post
categories: [interview]
---

##什么是KL散度？

##什么是算术平均、几何平均、调和平均？几何平均相比算数平均有什么优点

##不同的激活函数：Sigmoid、tanh、ReLU、ReLU6及变体P-R-Leaky、ELU、SELU、Swish、Mish、Maxout、hard-sigmoid、hard-swish、leaky relu(LRELU)、prelu、softplus

##平衡二叉树你会实现吗？

##离线auc提升与线上指标提升的比例关系？
目前的想法是1:10的关系，比如离线提升0.2pp，线上相关指标提升2%

##介绍下dropout？

##介绍下batch normalization? layernorm?

##训练数据有多少？qps有多少？

##tanh(双曲正切）公式怎么写？ sinh(双曲正弦）cosh(双曲余弦)呢 sigmoid呢

##讲一下batch normalization

##lstm公式推导一下

##rnn反向传播的公式推导一下

##常见的几种优化方法？

##参数的三种更新方式：batch mini-batch sgd 的区别是什么？

##attentional transformer是啥？

##a bottleneck structure是啥

## hierarchical softmax和adaptive softmax

## weight normalization是啥？

## Kaiming initialization?

## X, Y独立同分布 ～U(0, s)，求 E(max(X, Y))?

## X, Y独立同分布 ～N(e, d^2)，求 E(max(X, Y))?

## 多模态学习？

## RNN如何做反向传播

## 几种常见的优化方式：momentum adagrad adam等？
