---
layout: my_post
categories: [interview]
---

## 什么是KL散度？

## 什么是算术平均、几何平均、调和平均？几何平均相比算数平均有什么优点

## 不同的激活函数：Sigmoid、tanh、ReLU、ReLU6及变体P-R-Leaky、ELU、SELU、Swish、Mish、Maxout、hard-sigmoid、hard-swish、leaky relu(LRELU)、prelu、softplus

## 平衡二叉树你会实现吗？

## 离线auc提升与线上指标提升的比例关系？
目前的想法是1:10的关系，比如离线提升0.2pp，线上相关指标提升2%

## 介绍下dropout？

## 介绍下batch normalization? layernorm?

## 训练数据有多少？qps有多少？

## tanh(双曲正切）公式怎么写？ sinh(双曲正弦）cosh(双曲余弦)呢 sigmoid呢

## 讲一下batch normalization

## lstm公式推导一下

## rnn反向传播的公式推导一下

## 常见的几种优化方法？

## 参数的三种更新方式：batch mini-batch sgd 的区别是什么？

## attentional transformer是啥？

## a bottleneck structure是啥

## hierarchical softmax和adaptive softmax

## weight normalization是啥？

## Kaiming initialization?

## X, Y独立同分布 ～U(0, s)，求 E(max(X, Y))?

## X, Y独立同分布 ～N(e, d^2)，求 E(max(X, Y))?

## 多模态学习？

## RNN如何做反向传播

## 几种常见的优化方式：momentum adagrad adam等？

## 几个数据？
dau 总共6000w，br 2000w，in 1000w， spa 1300w；时长 br/in 60min，spa 50min
训练数据 每天84条训练数  evr:0.45 fpr:0.32 ctr:0.1 ltr:0.05
qps 高峰1.6w，低峰期4k
精排模型dense参数有1300w个，sparse slots总共有223个，加起来有5128维


## 雅可比矩阵
多个y，多个x，分别两两求偏导，横坐标对应y，纵坐标对应x

## 交叉熵怎么理解？
* 方法1: 与极大似然估计等价
* 方法2: kl散度(P|Q) = H(P,Q) - H(P)，P是真实分布，物理意义是使用基于Q的编码来编码来自P的样本，要多使用多少个bit。其中H(P, Q)就是交叉熵，我们要优化真实分布P和拟合分布Q之间的KL散度，而H(P)是保持不变的，因此可以直接优化H(P, Q)

## 互信息的公式：
* I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y)，注意这里的H(X,Y)是联合熵，与交叉熵用的一样的表达，但语境不同，代表的含义也不同
* 公式展开后是p(x,y)log(p(x,y)/p(x)/p(y))的求和

## 交叉熵和互信息
* 交叉熵/KL散度衡量的是两个分布的差异，就是事件有哪些是确定的，只是概率有差异
* 互信息衡量的是两个随机变量，互相依赖的程度。两个随机变量可以对应不同的事件。同时两个变量的联合分布，又是一些确定事件集合上的概率分布，因此I(X;Y)= KL散度(p(x,y) | p(x)\*p(y))

## Z-score Normalization 是啥？
* 就是减去均值，然后除以标准差
* 区别于min-max归一化

## 一篇讲逻辑回归的博客：[【机器学习】逻辑回归](https://zhuanlan.zhihu.com/p/74874291)
* sigmoid函数是怎么来的？出发点是使用线性模型回归对数几率，经过推导得到sigmoid表达式
* 代价函数就是对数似然函数
* 优化一般用梯度下降法，因为牛顿法涉及到求海森矩阵的逆矩阵
* 关于正则化，如何理解权重越小，模型就越简单？模型复杂的时候，输入值稍一变动，预测值就变动，这种情况w很小的时候比较难出现
* L1正则相当于给模型参数引入了拉普拉斯令均值先验分布，L2正则化相当于引入了零均值正态分布
* L2正则化引入loss函数的，其实是L2范数（L2 norm）的平方

## dropout不管是训练时对激活值除以p，还是预测时对W乘以p，本质上都是对激活值进行处理。。

## 根据史塔西的文章[先入为主：将先验知识注入推荐模型](https://zhuanlan.zhihu.com/p/442845759)来解释双塔粗排的一些网络设计
* 先验的重要特征用在两个地方：加在浅层（四大id拼在倒数第二层）以及当裁判（特征层面上的LHUC）
* 加在浅层的bucket等信息也能起到一定的debias作用
* evr向cstr等目标手动信息迁移

## 连续值按照分位数分桶也可以叫做：等频归一化

## 经典对比loss里为啥要设置margin？
* 不设置margin的话模型会走捷径：把所有的输入都映射到相同的输出上，这样loss就永远为0

## Metric Learning (http://contrib.scikit-learn.org/metric-learn/introduction.html)
* 什么是Metric Learning？
    * 全称Distance Metric Learning（距离元学习？），主要是学习points之间的距离。
* 监督学习和弱监督学习（weakly supervised learning）
    * 弱监督学习是指样本没有直接的label，而是有正样本对和负样本对，模型把正样本对的距离拉近，负样本对的距离拉远
* 输入是由feature组成的vector，其实可以理解为原始embedding。将原始embedding投影到一个新的空间中，在新的空间中的距离是符合学习目标的

## 非关系型数据库包括哪四种？key-value store, graph store, column store, document store

## 垂直扩展（vertical scaling）是指服务器数量不变，但是CPU和内存增加；水平扩展（horizontal scaling）是指增加服务器数量
* 垂直扩展比较简单，但是没有故障转移能力和冗余，同时也有硬件上限

## 负载均衡器（load balancer）不仅支持了后方server的水平扩展，也保护了server不与公网直接通信

## 数据库的主（master）从（slave）结构：主节点负责写入，从节点备份主节点数据并负责读出
* 这样做的好处是性能好，可靠性高

## 在server和db之间增加缓存（cache）能提升系统性能，但有几点需要注意：
* 过期时间不能太长或太短
* 跨区域的时候的数据一致性问题
* 缓存满了以后的逐出策略

## cdn提供的是静态文件：图片，视频，css，js脚本等

## server为了支持水平扩展，最好将user session相关的state数据存放在一个单独的共享的db中，将server本身搞成stateless的

## 时间的不同级别
* CPU/缓存操作在纳秒级别，nanosecond
* 内存/网络操作在微秒级别，microsecond
* 磁盘操作在毫秒级别，millisecond

## 一致性哈希：为的是server数量增加或减少时，减少不必要的key remapping
* 用的是hash ring的方式，将key和server在统一的空间中进行哈希映射，然后每个server负责一段区间内的key
* 为了防止分布不均匀，可以将每个server对应多个节点

## CAP定理
* 分布式系统无法同时满足3项：一致性（consistency），可用性（availability），分区容错性（partition tolerance）
