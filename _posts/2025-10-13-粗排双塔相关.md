---
layout: my_post
categories: [CV]
---

### 双塔模型如何解决多任务预估问题：[Embed Progressive Implicit Preference in Unified Space for Deep Collaborative Filtering](https://arxiv.org/pdf/2505.20900)
* 知乎：https://arxiv.org/pdf/2505.20900
* 将多个目标的预估融合进一套embedding中

### facebook的双塔召回：[负样本为王：评Facebook的向量化召回算法](https://zhuanlan.zhihu.com/p/165064102)
* 提到了双塔的loss：Pairwise Hinge Loss或者BPR loss
* 一个置信度高的离线评测手段仍然是召回工作中的痛点

### 粗排的几个发展方向，来自：https://zhuanlan.zhihu.com/p/681808861
* 对偶双塔
* 三塔,代表工作[https://zhuanlan.zhihu.com/p/611877625](https://zhuanlan.zhihu.com/p/611877625)
* DNN

### [腾讯的增强双塔:HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for Pre-Ranking Systems](https://arxiv.org/pdf/2505.19849)
* 包含了两个重要结构：生成器（generator）和多头表征（multi-head representation）
* 生成器解决特征交叉的问题
* 多头表征解决表征粒度的问题
* 此外还需要注意generator的输入和输出都stop gradient了
* 文章中还提到了业界的其他工作，有空可以看下

### [A Dual Augmented Two-tower Model for Online Large-scale Recommendation](https://dlp-kdd.github.io/assets/pdf/DLP-KDD_2021_paper_4.pdf)
* 美团的增强双塔，做法比较经典，就是直接去学对面的top embedding
* category alignment没啥意思

### [Mixture of virtual-kernel experts for multi-objective user profile modeling](https://arxiv.org/pdf/2106.07356)

### [Poly-encoders: architectures and pre-training strategies for fast and accurate multi-sentence scoring](https://arxiv.org/pdf/1905.01969)

### [IntTower: the Next Generation of Two-Tower Model for Pre-Ranking System](https://arxiv.org/pdf/2210.09890)

### [meta工作：Balancing Semantic Relevance and Engagement in Related Video Recommendations ](https://arxiv.org/pdf/2507.09403)
* 负采样缓解ssb问题
* 多模态（text+图片）特征加入网络
* label重新定义：要求正样本为语义相似并且发生点击行为的样本（因此它的优化场景是相似视频）
* IPW消除流行度偏差

### [LongRetriever: Towards Ultra-Long Sequence based Candidate Retrieval for Recommendation](https://arxiv.org/pdf/2508.15486v2)
* 字节出品，论文比较垃圾
* 从超长行为序列中根据垂类挑选出子序列，再参与训练，就是sim的做法
* 召回的时候，随机选一些垂类，每个垂类召回一些

### [Equip Pre-ranking with Target Attention by Residual Quantization](https://arxiv.org/pdf/2509.16931)
* 阿里的工作，整体比较繁杂
* 主要目的是要在双塔中赋予target attention的能力
* 如何既有target attention，同时又保留双塔user-item分开预估的能力？答：使用RQ-VAE，使用codebook机制代替TA，item的codebook编码随item emb提前算好
* 如何保证RQ-VAE能起到TA的作用？用蒸馏，离线学习加入target attention网络，指导RQ-VAE的学习
* RQ-VAE论文：[Autoregressive Image Generation using Residual Quantization](https://arxiv.org/pdf/2203.01941)

### [RankFlow: Joint Optimization of Multi-Stage Cascade Ranking Systems as Flows](https://www.ruizhang.info/publications/SIGIR%202022%20RankFlow%20-%20Joint%20Optimization%20of%20Multi-Stage%20Cascade%20Ranking.pdf)
* 以训练粗排模型为例，RankFlow包括三部分loss：
    * 在曝光集合上训练，作为warmup的方法
    * 在召回截断后的空间上训练，未曝光样本label 为0，曝光样本取真实label
    * 在召回截断后的空间上训练，采用精排模型做指导：精排模型先对所有样本打分（是的，没有进精排的也要打），然后通过rank loss学习精排的序，以及通过mse loss学习精排的值
* 评价：基本囊括了所有可能的并且合理的样本组织方式

### [Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems](https://arxiv.org/pdf/2310.10462)
* 主要解决什么问题？
    * 解决的是LTR问题
    * 首先在粗排模型上关注Recall比关注AUC/GAUC更合理，原因两点：1 粗排模型本身复杂度低，Recall学习起来比NDCG/OPA更容易 2 粗排本身的定位，也是筛选集合，而不是精准排序
    * 其次Recall指标本身不可微，需要代理loss，作者通过neural sort松弛置换矩阵（permutation matrix），得到一个代理loss
* 最后还加了一个loss，是通过neural sort松弛置换矩阵来优化OPA的
* 至于OPA 优化Loss和Recall 优化Loss分别的权重怎么设置，用的是UWL（Uncertainty Weight Loss）

### 双塔为什么要做l2 norm，以及为什么需要温度系数
* https://zhuanlan.zhihu.com/p/706479815

### 双塔做召回的经验
* https://zhuanlan.zhihu.com/p/705164965
