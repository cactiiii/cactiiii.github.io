---
layout: my_post
categories: [paper translate]
---

### 代码题整理，可用于快速检查思路
* https://github.com/doocs/leetcode/tree/main/lcof2/%E5%89%91%E6%8C%87%20Offer%20II%20016.%20%E4%B8%8D%E5%90%AB%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2

### 二分类任务评价指标
* [参考](https://zhuanlan.zhihu.com/p/669838554)
* 关键词：AUC（混淆矩阵），准确率，精准率/召回率，F-score

### 过拟合
* [参考](https://zhuanlan.zhihu.com/p/622943295)
* 如何确定过拟合？
* 如何缓解过拟合？
  * 数据：挖掘更多数据，数据增强，添加噪声
  * 特征：降维（PCA、T-SNE）、特征选择（Filter、WRAPPER、LHUC）
  * 模型: 集成模型、dropout、L1/L2正则化
  * 机制：early stop、K折交叉验证

### 梯度爆炸和梯度消失
* [参考](https://zhuanlan.zhihu.com/p/68579467)
* 梯度裁剪（按绝对值/按模长）
* 正则化（防止梯度爆炸）
* 选择合适的激活函数：Relu LeakyRelu PRelu
* 残差网络、* BN
* LSTM：专门解决RNN的问题

### 优化算法
* Adam
  * 惯性保持：一阶距
  * 环境感知：二阶距
  * 指数衰减平均
  * 初始值偏置矫正

### 这里需要注意我们为什么在计算得到 attention scores 之后需要对其进行缩放 scale 呢？
其中最重要的一点就是为了控制注意力权重的尺度，避免梯度爆炸和梯度消失。 因为对于点积而言，嵌入的维度越大，那么得到的 attention score 就容易会有极端值。这是因为，我们假设点积中每一个元素都是独立同分布的（例如均值为 0，方差为 1 的高斯分布）。 那么在点积之后的方差可以表示为 ：
Var(a*b) = Var(a1*b1 + a2*b2 + ... + aN*bN)
因为都是独立同分布，那么可以进一步成 
Var(a*b) = Var(a1*b1) + Var(a2*b2) + ... + Var(aN*bN)
而又有 
Var(a1*b1) = Var(a1)*Var(b1) + Var(a1)*E(b1^2) + Var(b1)*E(a1^2)
其中对于均值为 0，方差为 1 的分布，期望都为 0，因此Var(a1*b1) = 1,所以Var(a*b) = N ,即点积之后的方差与 embedding 的维度成正比。而期望的话因为都是零，所以没有改变。这就会导致点积的结果的绝对值会随着维度的增加而变大，从而导致在 softmax 后梯度变得极小或者极大，影响模型的训练。而如果我们除以 根号(N) 就能消除这种影响。

### 熵相关的概念
* [熵、联合熵、相对熵、交叉熵、JS散度、互信息、条件熵](https://developer.aliyun.com/article/1507693)
